{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the query plan II\n",
    "\n",
    "Suppose we want to join questions with users. We also want to apply a UDF (which performs some computation on the question's `body` field), and use a window function to order each user's questions by creation date.\n",
    "\n",
    "See the suboptimal query below that performs this task, and try to rewrite it to achieve a more efficient execution plan. Specifically, try to eliminate the Exchange operator from the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, udf, row_number\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Optimize II')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'data/questions-json')\n",
    "users_input_path = os.path.join(project_path, 'data/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will turn broadcast join off because we want to work with sort merge join (SMJ) because we want to assume that\n",
    "# in practice both datasets are large so SMJ would manifest anyway\n",
    "\n",
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.read.parquet(users_input_path)\n",
    "\n",
    "questionsDF = spark.read.json(questions_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF:\n",
    "\n",
    "The UDF bellow is just simple function that gets the lenght of a string. This can be easily done using native pyspark dataframe function length. For the sake of this example however suppose that this function encapsulates some complex logic which cannot be done natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(IntegerType())\n",
    "def get_length_udf(str):\n",
    "    return len(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().partitionBy('user_id').orderBy('creation_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .join(questionsDF, 'user_id')\n",
    "    .withColumn('question_len', get_length_udf('body'))\n",
    "    .withColumn('question_n', row_number().over(w))\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('noop')\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:\n",
    "\n",
    "The query above is suboptimal. Try to rewrite the query to achive more optimal plan that leads to more efficient execution.\n",
    "\n",
    "Hint:\n",
    "* see the query plan\n",
    "* eliminate the Exchange from the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
