{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics\n",
    "\n",
    "\n",
    "* build ML prototype that will predict if a question will be ansered in the next 30 minutes\n",
    "* model it as a binary classification\n",
    "* first prepare simple model with some basic features\n",
    "* then try to improve it by adding some more features\n",
    "* use random forest as a classifier\n",
    "* for modelling consider only questions that have accepted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, when, lit, length, array_sort, udf, desc\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler, Tokenizer, SQLTransformer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Predictive Analytics')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "answers_input_path = os.path.join(project_path, 'data/answers')\n",
    "\n",
    "users_input_path = os.path.join(project_path, 'data/users')\n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'data/questions-json')\n",
    "\n",
    "model_output_path = os.path.join(project_path, 'output/models/binary-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Load the data:</b>\n",
    "\n",
    "Hint:\n",
    "load all three datasets: anwers, questions, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', answers_input_path)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "questionsDF = (\n",
    "    spark\n",
    "    .read.format('json')\n",
    "    .option('path', questions_input_path)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "usersDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', users_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Add label to the dataset</b>\n",
    "\n",
    "hint:\n",
    "* join questions with answers on `accepted_anser_id`\n",
    "* join also with users on `user_id`\n",
    "* compute response time using unix_timestamp or cast the timestamps to long\n",
    "  * for questions you first need to cast the `creation_date` to timestamp if it is a string\n",
    "* use 'when' condition to compute the label (when response time <= 1800 then 1 otherwise 0)\n",
    "* cache the DataFrame for faster access in next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "data_with_label = (\n",
    "    questionsDF.withColumn('creation_date', col('creation_date').cast('timestamp')).alias('questions')\n",
    "    .join(answersDF.alias('answers'), questionsDF['accepted_answer_id'] == answersDF['answer_id'])\n",
    "    .join(usersDF.alias('users'), questionsDF['user_id'] == usersDF['user_id'])\n",
    "    .select(\n",
    "        col('questions.tags'),\n",
    "        col('questions.creation_date').alias('question_time'),\n",
    "        col('questions.title'),\n",
    "        col('questions.body').alias('message'),\n",
    "        col('answers.creation_date').alias('answer_time'),\n",
    "        col('users.reputation'),\n",
    "        col('users.upvotes'),\n",
    "        col('users.downvotes')\n",
    "    )\n",
    "    .withColumn('response_time', unix_timestamp('answer_time') - unix_timestamp('question_time'))\n",
    "    .withColumn('label', when(col('response_time') <= 1800, lit(1)).otherwise(0))\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_label.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Take a look at the distribution of classes</b>\n",
    "\n",
    "Hint\n",
    "* group by label and do the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "(\n",
    "    data_with_label\n",
    "    .groupBy('label')\n",
    "    .count()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Add some basic features:</b>\n",
    "\n",
    "hint:\n",
    "* add feature 'title_complexity'\n",
    " * compute the length of the question title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "data_with_basic_features = (\n",
    "    data_with_label\n",
    "    .withColumn('title_complexity', length('title'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare data</b>\n",
    "\n",
    "hint:\n",
    "* split the data for training and testing using [randomSplit](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html#pyspark.sql.DataFrame.randomSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "train_data, test_data = data_with_basic_features.randomSplit([0.7, 0.3], 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build the pipeline and train the model:</b>\n",
    "\n",
    "hint:\n",
    "* use: \n",
    " * [VectorAssembler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html#pyspark.ml.feature.VectorAssembler)\n",
    " * [RandomForestClassifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html#pyspark.ml.classification.RandomForestClassifier)\n",
    " * [Pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html#pyspark.ml.Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "features = ['title_complexity']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=(features), outputCol='features')\n",
    "\n",
    "# Classifier:\n",
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features', seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "rf_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluate the model</b>\n",
    "\n",
    "hint:\n",
    "* use [BinaryClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html#pyspark.ml.evaluation.BinaryClassificationEvaluator) with areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more features\n",
    "\n",
    "Hint:\n",
    "* add features: \n",
    "  * `question_size` number of words in the question body\n",
    "     * use [Tokenizer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html#pyspark.ml.feature.Tokenizer) to split the text on words\n",
    "     * use a [SQLTransformer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.SQLTransformer.html#pyspark.ml.feature.SQLTransformer) to compute the size\n",
    "    * you can try to add a bunch of other features such as reputation, upvotes, downvotes of the user and so on\n",
    "* train the model with this new pipeline\n",
    "* evaluate the model\n",
    "* see if the model improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeTrans = SQLTransformer(statement=\"SELECT *, size(words) AS message_size FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n",
    "features = ['title_complexity', 'message_size', 'reputation', 'upvotes', 'downvotes']\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='message', outputCol='words')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=(features), outputCol='features')\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features', seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, sizeTrans, assembler, rf])\n",
    "\n",
    "rf_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.stages[-1].getMaxDepth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the importance of the features\n",
    "\n",
    "Hint\n",
    "* access the last stage of the model to get the instance of the RandomForestClassificationModel\n",
    "  * use model.stages\n",
    "* see [featureImportances](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassificationModel.html#pyspark.ml.classification.RandomForestClassificationModel.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n",
    "importances = rf_model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, importance in zip(features, importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning:\n",
    "\n",
    "hint:\n",
    "* use [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html#pyspark.ml.tuning.ParamGridBuilder) to find optimal `numTrees` and optimal `maxDepth`\n",
    "* after you fit the [CrossValidator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html) access the best model as `cross_model.bestModel`\n",
    "* compute the accuracy using the evaluator on the predictions computed by the bestModel\n",
    "\n",
    "Note:\n",
    "\n",
    "If you run in local mode make the grid just 2 x 2 to avoid long run (3 x 3 can run over an hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (\n",
    "  ParamGridBuilder()\n",
    "  .addGrid(rf.maxDepth, [3, 5])\n",
    "  .addGrid(rf.numTrees, [50, 100])\n",
    "  .build()\n",
    ")\n",
    "\n",
    "cross_model = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid).fit(train_data)\n",
    "rf_model = cross_model.bestModel\n",
    "predictions = rf_model.transform(test_data)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the params of the model\n",
    "\n",
    "Hint\n",
    "* see `avgMetrics` of the cross_model\n",
    "* see the stages of the bestModel\n",
    "  * access the last stage to get the instance of the `RandomForestClassificationModel`\n",
    "* see:\n",
    "  * `getNumTrees`\n",
    "  * `getMaxDepth()`\n",
    "  * `toDebugString` to see full description of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n",
    "cross_model.bestModel.stages[-1].getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model.bestModel.stages[-1].getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model.bestModel.stages[-1].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model.bestModel.stages[-1].toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model so you can use it later in some ml application\n",
    "\n",
    "* use [write](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.PipelineModel.html#pyspark.ml.PipelineModel.write) on the [PipelineModel](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.PipelineModel.html#pyspark.ml.PipelineModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "(\n",
    "    rf_model\n",
    "    .write()\n",
    "    .overwrite()\n",
    "    .save(model_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
