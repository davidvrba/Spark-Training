{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d62f541-020c-4df1-9a32-fcd8a6680426",
   "metadata": {},
   "source": [
    "## Task\n",
    "In this notebook you will work with the spark-nlp library to find some information about the the `body` column from the questions dataset.\n",
    "\n",
    "* spark-nlp [docs](https://nlp.johnsnowlabs.com/docs/en/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1462a7-5538-4dfd-a808-cfef7b3e4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, col, sum, expr, desc, length\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import SentenceDetector, Tokenizer, NerConverter, WordEmbeddingsModel, PerceptronModel, NerCrfModel\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc22e6a-d569-4b4e-9587-1e0dc23ffa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('NLP I')\n",
    "    .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.3')\n",
    "    .config('spark.executor.memory', '20g')  # the memory is needed to run various parts of this notebook\n",
    "    .config('spark.driver.memory', '10g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253df071-7cf8-485c-aab3-62e4b45d93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "data_input_path = os.path.join(project_path, 'data/questions-json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229beb3-c1ba-4910-9058-16afe03e79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will take only small sample of the data (1%) to speed up the transformations\n",
    "\n",
    "dataDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('json')\n",
    "    .option('path', data_input_path)\n",
    "    .load()\n",
    "    .withColumnRenamed('title', 'Text')\n",
    "    .sample(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678fdac-88a2-4b34-888d-2d63cff5e759",
   "metadata": {},
   "source": [
    "## Compute the number of sentences in the dataset.\n",
    "### Hint\n",
    "* use [documentAssembler](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/document_assembler/index.html) as the entry point in the Spark NLP lib\n",
    "* use [sentenceDetector](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/sentence/sentence_detector/index.html) to split the text into sentences\n",
    "* use [Pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html) to specify both steps and fit it on the DataFrame to create a model\n",
    "* use the model to transform the DataFrame. This will add a new column of array type to the dataframe\n",
    "* use [size](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.size.html#pyspark.sql.functions.size) to compute number of elements in the array\n",
    "* sum the size accross the entire DataFrame using [agg](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html#pyspark.sql.DataFrame.agg) and [sum](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sum.html#pyspark.sql.functions.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ff76e-d2b3-4575-a02e-008647ee59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "documentAssembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol('Text')\n",
    "    .setOutputCol('document')\n",
    ")\n",
    "\n",
    "sentenceDetector = (\n",
    "    SentenceDetector()\n",
    "    .setInputCols('document')\n",
    "    .setOutputCol('sentence')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1d9b6-4722-4ff4-baab-1f24a8ad4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline().setStages([documentAssembler, sentenceDetector]).fit(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba093721-1291-43e6-b7ca-211eec069dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  model.transform(dataDF)\n",
    "  .withColumn('sentences', size('sentence'))\n",
    "  .agg(sum('sentences'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63eaa9-015c-4e3a-9cc4-2be356d64ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the schema of the dataframe transformed by the model:\n",
    "\n",
    "model.transform(dataDF).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079ee86-9dfa-41c2-a9ce-458636cf2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the extracted sentences:\n",
    "\n",
    "model.transform(dataDF).select('sentence').show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781648a-b405-4bdd-b31d-341b86956261",
   "metadata": {},
   "source": [
    "## Convert the `Text` column to tokens\n",
    "### Hint\n",
    "* use [Tokenizer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/token/tokenizer/index.html)\n",
    "* use [Pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html) and set the stages with the documentAssebler and Tokenizer\n",
    "* fit the pipeline on the DataFrame to create a model\n",
    "* use the model to transform the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6e818-0c1b-47c4-8cff-c2f2a05c2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = (\n",
    "    Tokenizer()\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('token')\n",
    ")\n",
    "\n",
    "model = Pipeline().setStages([documentAssembler, tokenizer]).fit(dataDF)\n",
    "\n",
    "model.transform(dataDF).select('token').show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79880e-0b5d-491a-9e87-8df30fb2a33a",
   "metadata": {},
   "source": [
    "# Compute NER (Named Entity Recognition)\n",
    "\n",
    "Hint:\n",
    "* compute POS (part-of-speech tags)\n",
    "  * use `PerceptronModel.pretrained(\"pos_anc\", \"en\")`\n",
    "  * see [docs](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/pos/perceptron/index.html#sparknlp.annotator.pos.perceptron.PerceptronModel)\n",
    "* compute embeddings\n",
    "  * use `WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\")`\n",
    "  * see [docs](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/embeddings/word_embeddings/index.html#sparknlp.annotator.embeddings.word_embeddings.WordEmbeddingsModel)\n",
    "* compute NER\n",
    "  * use `NerCrfModel.pretrained(\"ner_crf\", \"en\")`\n",
    "  * see [docs](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/ner/ner_crf/index.html#sparknlp.annotator.ner.ner_crf.NerCrfModel)\n",
    "  * use [NerConverter](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/ner/ner_converter/index.html#sparknlp.annotator.ner.ner_converter.NerConverter) to convert the data to user-friendly shape\n",
    "  * build Pipeline\n",
    "  * fit the data to create a model\n",
    "  * transform the data using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a2ecf-e543-4e37-ae8d-8a594e3860c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagger:\n",
    "pos_tagger = (\n",
    "    PerceptronModel.pretrained('pos_anc', 'en') \n",
    "    .setInputCols(['document', 'token']) \n",
    "    .setOutputCol('pos')\n",
    ")\n",
    "\n",
    "# WordEmbeddings:\n",
    "embeddings = (\n",
    "    WordEmbeddingsModel.pretrained('glove_100d', 'en') \n",
    "    .setInputCols(['document', 'token']) \n",
    "    .setOutputCol('word_embeddings')\n",
    ")\n",
    "\n",
    "# NerCrfModel:\n",
    "ner = (\n",
    "    NerCrfModel.pretrained('ner_crf', 'en') \n",
    "    .setInputCols(['document', 'token', 'pos', 'word_embeddings']) \n",
    "    .setOutputCol('ner')\n",
    ")\n",
    "\n",
    "# NerConverter:\n",
    "ner_converter = (\n",
    "    NerConverter()\n",
    "    .setInputCols(['document', 'token', 'ner'])\n",
    "    .setOutputCol('entities')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915076ec-8c89-4eb2-aa4b-bbc948b0278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    pos_tagger,\n",
    "    embeddings,\n",
    "    ner,\n",
    "    ner_converter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702dee8-fb99-4fdb-83c7-69e66c288432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline to the data to create the model\n",
    "\n",
    "model = pipeline.fit(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622b2ab-168a-46ec-b9e9-7098174a50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data using the model\n",
    "\n",
    "results = model.transform(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0137f11-6121-4722-bf00-fb6c78d1ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the result\n",
    "\n",
    "(\n",
    "    results.select('entities')\n",
    ").show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7dceb-da7a-4109-896c-82850f1ba4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
