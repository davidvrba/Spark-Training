{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e9bb82-828d-4141-9a60-34d32c30d3d3",
   "metadata": {},
   "source": [
    "## Find the user that used apache-spark tag the most times. Also find out how many times he used all other tags.\n",
    "\n",
    "In this notebook you will use higher order functions to analyze the tag usage by users that asked questions.\n",
    "\n",
    "Answer to this question should have this format: \n",
    "* The user that used `apache-spark` tag most frequently has id=xxx\n",
    "* He use the tag xxx times\n",
    "* Here is the frequency of all other tags he used:\n",
    "```\n",
    "{\n",
    "  'hadoop': x,\n",
    "  'sql': y,\n",
    "  'python': z, # similarly for all other tags he used\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcf8aa-c06f-419c-a413-274dd9a9614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, collect_list, lit, \n",
    "    concat, flatten, length, aggregate, map_concat, create_map, coalesce, map_contains_key, desc, count\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9c539-a37d-4622-a2b8-6a6a4393ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Higher Order Functions II')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3e860-ba22-4005-bae1-5065cf5540c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "questions_json_input_path = os.path.join(project_path, 'data/questions-json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c646a93-f0bf-4f7c-940b-24dd01e26e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('json')\n",
    "    .option('path', questions_json_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac160753-0d7c-48e7-a822-af917f6b6958",
   "metadata": {},
   "source": [
    "### Split the tags into an array and find out what tags were used by each user\n",
    "\n",
    "Hint:\n",
    "* check the tags column, it is in this format `<tag1><tag2><tag3>`\n",
    "* convert the tags into an array: [tag1, tag2, tag3, ...]\n",
    "  * there are different ways how to do it, you will need split + some other technique how to remove the angle brackets\n",
    "* group by user and collect all tags into single array\n",
    "  * check [flatten](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.flatten.html) to handle nested arrays\n",
    "* output DataFrame should contain 2 columns: user_id, tags where tags is an array of all tags used by the user (some of the text will repeat, which is good, we will count them in the next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d50c5-45bc-4a8b-9a6b-ddf00d2d9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tags = (\n",
    "    questionsDF\n",
    "    .withColumn('fixed_tags', col('tags').substr(lit(2), length('tags') - 2))\n",
    "    .withColumn('fixed_tags', split('fixed_tags', '><'))\n",
    "    .groupBy('user_id')\n",
    "    .agg(\n",
    "        flatten(collect_list('fixed_tags')).alias('tags')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0ca60-6e68-455d-aa93-82f79fc09673",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tags.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d3744-e5da-4825-a392-6943981eef4e",
   "metadata": {},
   "source": [
    "### Count the frequency of the tags for each user\n",
    "\n",
    "Hint (this is the more complex part of the task)\n",
    "* Use a MapType to store the tags as follows:\n",
    "  * tag1->4, tag2->1, tag3->10, i.e the key is the name of the tag and the value is its frequency\n",
    "  * you will need [aggregate](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.aggregate.html), [create_map](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.create_map.html), [map_concat](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_concat.html)\n",
    "  * you may need to set [spark.sql.mapKeyDedupPolicy](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L4709) config property to LAST_WIN to deal with duplicated keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71089a7d-6e88-4498-a64a-371af3a6042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.mapKeyDedupPolicy', \"LAST_WIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb3a03-fa00-4217-8250-c85ccee7e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_with_frequency = (\n",
    "    user_tags\n",
    "    .withColumn(\n",
    "        'tags',\n",
    "        aggregate(\n",
    "            'tags',\n",
    "            create_map().cast(MapType(StringType(), IntegerType())),\n",
    "            lambda acc, x: map_concat(acc, create_map(x, coalesce(acc[x], lit(0)) + 1))\n",
    "        )\n",
    "    )    \n",
    ")\n",
    "\n",
    "tags_with_frequency.show(n=5, truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de18f31-2607-4bcb-a467-15bbcd296935",
   "metadata": {},
   "source": [
    "### Find the user that used apache-spark tag with highest frequency\n",
    "\n",
    "Hint:\n",
    "* filter the DataFrame using map_contains_key to get users that used the particular tag\n",
    "* add the frequncy of the particular tag to a new column and sort the DataFrame by it to find the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9a440-d6ad-4616-8d0c-d5122a5286bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_spark_tag = (\n",
    "    tags_with_frequency\n",
    "    .filter(map_contains_key('tags', 'apache-spark'))\n",
    "    .withColumn('spark_frequency_tag', col('tags')['apache-spark'])\n",
    "    .orderBy(desc('spark_frequency_tag'))\n",
    ")\n",
    "\n",
    "users_with_spark_tag.show(n=5, truncate=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d7738-66e9-4aea-ba3e-36577e613bd9",
   "metadata": {},
   "source": [
    "### Find the frequency of all other tags for the user\n",
    "\n",
    "Hint:\n",
    "* just collect the row - it should contain all the tags already, the MapType will become Python dictionary of all tags used by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63541e64-46b9-4069-adc8-e3ee0e71cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    \n",
    "    users_with_spark_tag\n",
    "    .limit(1)\n",
    "    .select('user_id', 'tags')\n",
    ").collect()[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156314f-bd57-4ef5-a84c-a9d5ec0a48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
